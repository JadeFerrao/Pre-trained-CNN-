# Pretrained Deep Learning Architectures
This repository contains various popular deep learning architectures implemented in TensorFlow/Keras. Below is a brief description of each architecture included in this repository: 

# Inception Net

Inception Net, also known as GoogLeNet, is a deep convolutional neural network architecture that was the winner of the ILSVRC 2014 competition. It introduces the inception module, which dramatically reduces the number of parameters in the network while maintaining performance.  

# MobileNet

MobileNet is a class of efficient models optimized for mobile and embedded vision applications. It uses depthwise separable convolutions to build light-weight deep neural networks, making it suitable for mobile and embedded devices with limited computational power.  

# ResNet252

ResNet, or Residual Network, is a widely-used deep learning architecture that introduces residual learning via shortcut connections. While the standard versions are ResNet50, ResNet101, and ResNet152, this repository includes ResNet252, which is an extended version of these models with more layers to capture more complex patterns.  

# EfficientNet

EfficientNet is a model scaling approach that uniformly scales all dimensions of depth, width, and resolution using a compound coefficient. It has been shown to achieve better accuracy and efficiency than previous models by systematically balancing these three dimensions.  

# DenseNet121

DenseNet, or Densely Connected Convolutional Network, connects each layer to every other layer in a feed-forward fashion. DenseNet121 is one of the standard configurations, known for requiring fewer parameters and alleviating the vanishing-gradient problem.  

# XceptionNet

XceptionNet is an extension of the Inception architecture which replaces the standard Inception modules with depthwise separable convolutions. This results in a model that is both more efficient and more powerful.  

# Inception-ResNet

Inception-ResNet combines the inception modules with residual connections, aiming to benefit from both the inception architecture's multi-scale feature extraction and the residual connections' ability to ease the training of very deep networks.  
